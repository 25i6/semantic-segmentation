{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/25i6/semantic-segmentation/blob/main/notebooks/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryAs3oIExsIh"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCc6lRgHxsIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558f7d38-8eb4-4ec2-9e6a-cb6075c2b968",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'semantic-segmentation'...\n",
            "remote: Enumerating objects: 801, done.\u001b[K\n",
            "remote: Counting objects: 100% (798/798), done.\u001b[K\n",
            "remote: Compressing objects: 100% (317/317), done.\u001b[K\n",
            "remote: Total 801 (delta 485), reused 756 (delta 474), pack-reused 3 (from 1)\u001b[K\n",
            "Receiving objects: 100% (801/801), 54.99 MiB | 28.18 MiB/s, done.\n",
            "Resolving deltas: 100% (485/485), done.\n",
            "/content/semantic-segmentation\n",
            "Obtaining file:///content/semantic-segmentation\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from semseg==0.4.1) (4.67.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from semseg==0.4.1) (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from semseg==0.4.1) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from semseg==0.4.1) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from semseg==0.4.1) (3.10.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from semseg==0.4.1) (2.18.0)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sithu31296/semantic-segmentation\n",
        "%cd semantic-segmentation\n",
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgYwr_rPxsIr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import io\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "def show_image(image):\n",
        "    if image.shape[2] != 3: image = image.permute(1, 2, 0)\n",
        "    image = Image.fromarray(image.numpy())\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4DiMMeGxsIt"
      },
      "source": [
        "## Show Available Pretrained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18fpjEzbxsIu"
      },
      "outputs": [],
      "source": [
        "from semseg import show_models\n",
        "\n",
        "show_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BlLFsLRxsIu"
      },
      "source": [
        "## Load a Pretrained Model\n",
        "\n",
        "Download a pretrained model's weights from the result table (ADE20K, CityScapes, ...) and put it in `checkpoints/pretrained/model_name/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5XuGJC2xsIv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install -U gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ4bHWupxsIw"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "from pathlib import Path\n",
        "\n",
        "ckpt = Path('./checkpoints/pretrained/segformer')\n",
        "ckpt.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1-OmW3xRD3WAbJTzktPC-VMOF5WMsN8XT'\n",
        "output = './checkpoints/pretrained/segformer/segformer.b3.ade.pth'\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl8Dy39wxsIw"
      },
      "outputs": [],
      "source": [
        "from semseg.models import *\n",
        "\n",
        "model = eval('SegFormer')(\n",
        "    backbone='MiT-B3',\n",
        "    num_classes=150\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load('checkpoints/pretrained/segformer/segformer.b3.ade.pth', map_location='cpu'))\n",
        "except:\n",
        "    print(\"Download a pretrained model's weights from the result table.\")\n",
        "model.eval()\n",
        "\n",
        "print('Loaded Model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_KHRyNwxsIx"
      },
      "source": [
        "## Simple Image Inference\n",
        "\n",
        "### Load Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qi60yJT81UBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKZdVXgixsIx"
      },
      "outputs": [],
      "source": [
        "image_path = '/content/drive/MyDrive/2025autum/02_Special_Laboratory/02_kosode/junk_box/14-D-201350-100-0003.jpg'#最後がjpgとかじゃなくて日本語だと一個下のブロックでエラー\n",
        "image = io.read_image(image_path)\n",
        "print(image.shape)\n",
        "show_image(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4R4S7-sxsIy"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xYzGrUUxsIy"
      },
      "outputs": [],
      "source": [
        "# resize\n",
        "image = T.CenterCrop((512, 512))(image)\n",
        "# scale to [0.0, 1.0]\n",
        "image = image.float() / 255\n",
        "# normalize\n",
        "image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(image)\n",
        "# add batch size\n",
        "image = image.unsqueeze(0)\n",
        "image.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上のコメントアウトなし版．"
      ],
      "metadata": {
        "id": "oqEldeVIo93n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fK7jILkhSzv"
      },
      "outputs": [],
      "source": [
        "image = T.CenterCrop((512, 512))(image)\n",
        "image = image.float() / 255\n",
        "image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(image)\n",
        "image = image.unsqueeze(0)\n",
        "image.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQfK-5mxxsIz"
      },
      "source": [
        "### Model Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6e7XBIQxsIz"
      },
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n",
        "    seg = model(image)\n",
        "seg.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f04UkDYJxsIz"
      },
      "source": [
        "### Postprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDgZjYR6xsI0"
      },
      "outputs": [],
      "source": [
        "seg = seg.softmax(1).argmax(1).to(int)\n",
        "#\n",
        "seg_unique = seg.unique()\n",
        "seg.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOAl-oV4xsI0"
      },
      "outputs": [],
      "source": [
        "from semseg.datasets import *\n",
        "\n",
        "palette = eval('ADE20K').PALETTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQlfE-XTxsI0"
      },
      "outputs": [],
      "source": [
        "seg_map = palette[seg].squeeze().to(torch.uint8)\n",
        "show_image(seg_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "評価"
      ],
      "metadata": {
        "id": "LptRynSU2z6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "#import numpy as np\n",
        "#import torch\n",
        "import cv2\n",
        "#import matplotlib.pyplot as plt\n",
        "#from semseg.datasets import ADE20K\n",
        "\n",
        "def load_labelme_json(json_file):\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def compute_metrics(seg, gt):\n",
        "    pixel_accuracy = (seg == gt).sum() / gt.size\n",
        "    intersection = np.logical_and(seg == gt, gt > 0).sum()\n",
        "    union = np.logical_or(seg > 0, gt > 0).sum()\n",
        "    miou = intersection / (union + 1e-10)\n",
        "    return pixel_accuracy, miou\n",
        "\n",
        "def visualize_segmentation(seg, palette):\n",
        "    seg_map = np.array(palette)[seg.squeeze()]\n",
        "    return seg_map.astype(np.uint8)\n",
        "\n",
        "def overlay_labels(image, seg, seg_unique):\n",
        "    for label in seg_unique:\n",
        "        mask = (seg == label)\n",
        "        y, x = np.where(mask)\n",
        "        if len(x) > 0 and len(y) > 0:\n",
        "            cx, cy = int(x.mean()), int(y.mean())\n",
        "            cv2.putText(image, str(label), (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "def main(json_file, seg_tensor):\n",
        "    labelme_data = load_labelme_json(json_file)\n",
        "    gt = np.array(labelme_data['shapes'])\n",
        "    seg = seg_tensor.softmax(1).argmax(1).cpu().numpy()\n",
        "    seg_unique = np.unique(seg)\n",
        "    palette = ADE20K.PALETTE\n",
        "    seg_map = visualize_segmentation(seg, palette)\n",
        "    map_unique = np.unique(seg_map.reshape(-1, seg_map.shape[-1]), axis=0)\n",
        "    seg_map = overlay_labels(seg_map, seg, seg_unique)\n",
        "    pixel_accuracy, miou = compute_metrics(seg, gt)\n",
        "    print(f'Pixel Accuracy: {pixel_accuracy:.4f}, mIoU: {miou:.4f}')\n",
        "    plt.imshow(seg_map)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# 使用例（適切な json_file と seg_tensor を指定する）\n",
        "main(\"/content/drive/MyDrive/2025autum/02_Special_Laboratory/02_kosode/junk_box/14-D-201350-100-0057.json\", seg)\n"
      ],
      "metadata": {
        "id": "lhyVtVKO21qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "追加"
      ],
      "metadata": {
        "id": "Agvjbz-48DI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from semseg.datasets import ADE20K  # 例として ADE20K を利用\n",
        "\n",
        "# -------------------------\n",
        "# ① セグメンテーション結果の後処理\n",
        "# -------------------------\n",
        "# ※ seg はネットワークの出力テンソルとします（例: [B, num_classes, H, W]）\n",
        "#    ここではバッチサイズ1を前提としています。\n",
        "\n",
        "# # 各ピクセルのクラスを softmax と argmax で求める\n",
        "# seg = seg.softmax(1).argmax(1).to(torch.int)  # seg の shape は [1, H, W] になることを想定\n",
        "# seg_unique = seg.unique()  # 画像内に現れるユニークなクラス番号\n",
        "\n",
        "# -------------------------\n",
        "# ② カラーパレットを用いたセグメンテーションマップの生成\n",
        "# -------------------------\n",
        "# ADE20K のパレット（色リスト）を取得\n",
        "palette = ADE20K.PALETTE\n",
        "# クラス名が定義されている場合（ADE20K.CLASSES など）：\n",
        "classes = ADE20K.CLASSES if hasattr(ADE20K, 'CLASSES') else None\n",
        "\n",
        "# seg の各画素に対してパレットの色を適用\n",
        "# ※ palette[seg] の結果はテンソルで、形状は [1, H, W, 3] となることを想定\n",
        "seg_map = palette[seg].squeeze().to(torch.uint8)  # shape: [H, W, 3]\n",
        "\n",
        "# -------------------------\n",
        "# ③ NumPy 配列に変換して各クラス領域に注釈を描画\n",
        "# -------------------------\n",
        "# テンソルを NumPy 配列へ変換（以降 OpenCV の描画処理のため）\n",
        "seg_np = seg.squeeze().cpu().numpy()      # [H, W]（クラス番号）\n",
        "seg_map_np = seg_map.cpu().numpy()          # [H, W, 3]（カラー画像）\n",
        "\n",
        "# 画像のコピーを用意（注釈描画用）\n",
        "annotated_img = seg_map_np.copy()\n",
        "\n",
        "# ユニークな各クラス番号について処理\n",
        "for cls in seg_unique.tolist():\n",
        "    # 該当クラスのマスク作成\n",
        "    mask = (seg_np == cls)\n",
        "    if np.sum(mask) == 0:\n",
        "        continue\n",
        "\n",
        "    # マスク上のピクセル座標を取得\n",
        "    coords = np.argwhere(mask)\n",
        "    # 領域の左上と右下の座標（バウンディングボックス）を算出\n",
        "    y0, x0 = coords.min(axis=0)\n",
        "    y1, x1 = coords.max(axis=0)\n",
        "    # バウンディングボックスの中心点を求める\n",
        "    center_y, center_x = int((y0 + y1) / 2), int((x0 + x1) / 2)\n",
        "\n",
        "    # クラス番号と、可能ならタグ名（クラス名）をテキストに\n",
        "    label_text = f\"{cls}\"\n",
        "    if classes is not None and cls < len(classes):\n",
        "        label_text += f\": {classes[cls]}\"\n",
        "\n",
        "    # OpenCV の putText 関数でテキストを描画\n",
        "    # 第4引数はフォント、第5引数はフォントサイズ、(255,255,255) はテキスト色（白）\n",
        "    cv2.putText(annotated_img, label_text, (center_x, center_y),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255),\n",
        "                thickness=1, lineType=cv2.LINE_AA)\n",
        "\n",
        "# -------------------------\n",
        "# ④ 結果の表示\n",
        "# -------------------------\n",
        "# ここでは show_image 関数を使って画像を表示します（実装は各自）\n",
        "show_image(torch.from_numpy(annotated_img))\n"
      ],
      "metadata": {
        "id": "Fores9WU8C4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTphPtElxsI1"
      },
      "source": [
        "## Show Available Backbones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueZJFbh6xsI1"
      },
      "outputs": [],
      "source": [
        "from semseg import show_backbones\n",
        "\n",
        "show_backbones()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tu8EPnrxsI1"
      },
      "source": [
        "## Show Available Heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7otmbi8dxsI1"
      },
      "outputs": [],
      "source": [
        "from semseg import show_heads\n",
        "\n",
        "show_heads()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB1RJwrIxsI2"
      },
      "source": [
        "## Show Available Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXLIha2rxsI2"
      },
      "outputs": [],
      "source": [
        "from semseg import show_datasets\n",
        "\n",
        "show_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIBa0c21xsI2"
      },
      "source": [
        "## Construct a Custom Model\n",
        "\n",
        "### Choose a Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7jqH3Q-xsI2"
      },
      "outputs": [],
      "source": [
        "from semseg.models.backbones import ResNet\n",
        "\n",
        "backbone = ResNet('18')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZDBV_XwxsI3"
      },
      "outputs": [],
      "source": [
        "# init random input batch\n",
        "x = torch.randn(2, 3, 224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4DdCLapxsI3"
      },
      "outputs": [],
      "source": [
        "# get features from the backbone\n",
        "features = backbone(x)\n",
        "for out in features:\n",
        "    print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koW9UlCZxsI3"
      },
      "source": [
        "### Choose a Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEwHG7MXxsI4"
      },
      "outputs": [],
      "source": [
        "from semseg.models.heads import UPerHead\n",
        "\n",
        "head = UPerHead(backbone.channels, 128, num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pXWhEPexsI4"
      },
      "outputs": [],
      "source": [
        "seg = head(features)\n",
        "seg.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_voj3kwNxsI4"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "# upsample the output\n",
        "seg = F.interpolate(seg, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
        "seg.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdC8WODJxsI4"
      },
      "source": [
        "Check `semseg/models/custom_cnn.py` and `semseg/models/custom_vit.py` for a complete construction for custom model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bggXwqgfxsI5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}